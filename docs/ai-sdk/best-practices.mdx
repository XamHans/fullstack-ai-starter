---
title: "AI SDK Best Practices"
description: "Production-ready strategies for AI text generation. Learn optimization techniques, cost management, security practices, and performance patterns for scalable AI applications."
---

## Performance Optimization

### Response Time Optimization

<CardGroup cols={2}>
  <Card
    title="Model Selection"
    icon="cpu"
  >
    Choose the right model for your use case balance between quality and speed.
  </Card>
  <Card
    title="Prompt Engineering"
    icon="edit"
  >
    Optimize prompts for faster, more accurate responses.
  </Card>
  <Card
    title="Caching Strategy"
    icon="database"
  >
    Implement intelligent caching to reduce API calls and improve response times.
  </Card>
  <Card
    title="Streaming Responses"
    icon="zap"
  >
    Use streaming for better user experience with long-form content.
  </Card>
</CardGroup>

### Model Selection Strategy

Choose models based on your specific requirements:

<Tabs>
  <Tab title="Development & Prototyping">
    ```typescript
    // Fast iteration with cost-effective model
    const { text } = await generateText({
      model: openai('gpt-3.5-turbo'),
      prompt: userInput,
      maxTokens: 500, // Limit for development
      temperature: 0.7
    });
    ```

    **Best for:**
    - Initial development
    - A/B testing prompts
    - High-volume, simple tasks
    - Cost-sensitive applications
  </Tab>

  <Tab title="Production Quality">
    ```typescript
    // High-quality model for production
    const { text } = await generateText({
      model: openai('gpt-4'),
      prompt: optimizedPrompt,
      maxTokens: 2000,
      temperature: 0.3, // More consistent
      systemInstruction: brandVoiceInstruction
    });
    ```

    **Best for:**
    - Customer-facing content
    - Complex reasoning tasks
    - Brand-critical communications
    - High-stakes applications
  </Tab>

  <Tab title="Balanced Approach">
    ```typescript
    // Model switching based on content type
    function selectModel(contentType: string, priority: 'speed' | 'quality') {
      if (priority === 'speed' || contentType === 'simple') {
        return openai('gpt-3.5-turbo');
      }

      if (contentType === 'creative' || contentType === 'analysis') {
        return anthropic('claude-3-sonnet-20240229');
      }

      return openai('gpt-4');
    }

    const model = selectModel(request.contentType, request.priority);
    ```
  </Tab>
</Tabs>

### Intelligent Caching

<Accordion title="Cache Frequently Requested Content">
  Implement caching for common prompts and reduce API calls:

  ```typescript
  import { unstable_cache } from 'next/cache';
  import { generateText } from 'ai';
  import { openai } from '@ai-sdk/openai';

  // Cache based on prompt hash
  const generateCachedText = unstable_cache(
    async (prompt: string, model: string) => {
      const { text } = await generateText({
        model: openai(model),
        prompt
      });
      return text;
    },
    ['ai-generate-text'],
    {
      revalidate: 3600, // Cache for 1 hour
      tags: ['ai-content']
    }
  );

  // Usage with cache hit detection
  export async function generateWithCache(
    prompt: string,
    options: { model?: string; forceRefresh?: boolean } = {}
  ) {
    const cacheKey = `${prompt}-${options.model || 'gpt-3.5-turbo'}`;

    if (options.forceRefresh) {
      // Bust cache for this specific request
      revalidateTag('ai-content');
    }

    return generateCachedText(prompt, options.model || 'gpt-3.5-turbo');
  }
  ```
</Accordion>

<Accordion title="Smart Cache Invalidation">
  ```typescript
  // Cache with automatic invalidation
  class AIContentCache {
    private cache = new Map<string, {
      content: string;
      timestamp: number;
      usage: number;
    }>();

    private getCacheKey(prompt: string, model: string): string {
      return `${model}:${this.hashPrompt(prompt)}`;
    }

    private hashPrompt(prompt: string): string {
      // Simple hash - use crypto.createHash in production
      return Buffer.from(prompt).toString('base64').slice(0, 32);
    }

    async get(prompt: string, model: string): Promise<string | null> {
      const key = this.getCacheKey(prompt, model);
      const cached = this.cache.get(key);

      if (cached) {
        const age = Date.now() - cached.timestamp;
        const maxAge = cached.usage > 10 ? 7200000 : 3600000; // 2h if popular, 1h otherwise

        if (age < maxAge) {
          cached.usage++;
          return cached.content;
        }

        this.cache.delete(key);
      }

      return null;
    }

    set(prompt: string, model: string, content: string): void {
      const key = this.getCacheKey(prompt, model);
      this.cache.set(key, {
        content,
        timestamp: Date.now(),
        usage: 1
      });
    }

    // Periodic cleanup
    cleanup(): void {
      const now = Date.now();
      for (const [key, value] of this.cache) {
        if (now - value.timestamp > 7200000) { // 2 hours
          this.cache.delete(key);
        }
      }
    }
  }
  ```
</Accordion>

### Streaming for Better UX

Implement streaming for immediate user feedback:

```typescript
import { streamText } from 'ai';

export async function generateStreamingText(prompt: string) {
  const result = streamText({
    model: openai('gpt-4'),
    prompt,
  });

  return result;
}

// Client-side usage
function StreamingTextGenerator() {
  const [content, setContent] = useState('');
  const [isGenerating, setIsGenerating] = useState(false);

  const handleGenerate = async () => {
    setIsGenerating(true);
    setContent('');

    try {
      const response = await fetch('/api/stream-text', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ prompt: 'Write about AI...' }),
      });

      const reader = response.body?.getReader();
      const decoder = new TextDecoder();

      while (true) {
        const { value, done } = await reader!.read();
        if (done) break;

        const chunk = decoder.decode(value);
        setContent(prev => prev + chunk);
      }
    } catch (error) {
      console.error('Streaming error:', error);
    } finally {
      setIsGenerating(false);
    }
  };

  return (
    <div>
      <Button onClick={handleGenerate} disabled={isGenerating}>
        Generate Stream
      </Button>

      <div className="mt-4">
        {content}
        {isGenerating && <span className="animate-pulse">|</span>}
      </div>
    </div>
  );
}
```

## Cost Management

### Token Usage Optimization

<Warning>
Monitor and optimize token usage to control costs effectively.
</Warning>

<Accordion title="Estimate Costs Before Generation">
  ```typescript
  interface CostEstimation {
    inputTokens: number;
    estimatedOutputTokens: number;
    estimatedCost: number;
    model: string;
  }

  function estimateGenerationCost(
    prompt: string,
    maxTokens: number,
    model: string
  ): CostEstimation {
    // Rough token estimation (1 token â‰ˆ 0.75 words)
    const inputTokens = Math.ceil(prompt.split(' ').length / 0.75);
    const estimatedOutputTokens = maxTokens;

    const pricing = {
      'gpt-3.5-turbo': { input: 0.001, output: 0.002 }, // per 1k tokens
      'gpt-4': { input: 0.01, output: 0.03 },
      'gpt-4-turbo': { input: 0.01, output: 0.03 }
    };

    const modelPricing = pricing[model] || pricing['gpt-3.5-turbo'];
    const estimatedCost =
      (inputTokens / 1000) * modelPricing.input +
      (estimatedOutputTokens / 1000) * modelPricing.output;

    return {
      inputTokens,
      estimatedOutputTokens,
      estimatedCost,
      model
    };
  }

  // Usage with cost limiting
  export async function generateWithCostControl(
    prompt: string,
    options: {
      maxCost?: number;
      model?: string;
      maxTokens?: number;
    } = {}
  ) {
    const estimation = estimateGenerationCost(
      prompt,
      options.maxTokens || 1000,
      options.model || 'gpt-3.5-turbo'
    );

    if (options.maxCost && estimation.estimatedCost > options.maxCost) {
      throw new Error(`Estimated cost (${estimation.estimatedCost}) exceeds limit (${options.maxCost})`);
    }

    return generateText({
      model: openai(estimation.model),
      prompt,
      maxTokens: options.maxTokens
    });
  }
  ```
</Accordion>

<Accordion title="Implement Usage Tracking">
  ```typescript
  // Track usage per user/organization
  class UsageTracker {
    async trackGeneration(
      userId: string,
      model: string,
      inputTokens: number,
      outputTokens: number,
      cost: number
    ) {
      await db.usage.create({
        data: {
          userId,
          model,
          inputTokens,
          outputTokens,
          cost,
          timestamp: new Date()
        }
      });
    }

    async getUserUsage(userId: string, period: 'day' | 'month' = 'month') {
      const since = period === 'day'
        ? new Date(Date.now() - 24 * 60 * 60 * 1000)
        : new Date(Date.now() - 30 * 24 * 60 * 60 * 1000);

      return db.usage.aggregate({
        where: {
          userId,
          timestamp: { gte: since }
        },
        _sum: {
          inputTokens: true,
          outputTokens: true,
          cost: true
        },
        _count: true
      });
    }

    async checkUsageLimits(userId: string): Promise<boolean> {
      const usage = await this.getUserUsage(userId, 'month');
      const limit = await this.getUserLimit(userId);

      return (usage._sum.cost || 0) < limit.monthlyCostLimit;
    }
  }
  ```
</Accordion>

### Budget Controls

Implement spending controls to prevent cost overruns:

```typescript
// Middleware for cost control
async function withCostControl(
  handler: Function,
  userId: string,
  estimatedCost: number
) {
  const usage = await usageTracker.getUserUsage(userId, 'month');
  const userLimits = await getUserLimits(userId);

  // Check monthly budget
  if ((usage._sum.cost || 0) + estimatedCost > userLimits.monthlyBudget) {
    throw new Error('Monthly budget exceeded');
  }

  // Check daily budget
  const dailyUsage = await usageTracker.getUserUsage(userId, 'day');
  if ((dailyUsage._sum.cost || 0) + estimatedCost > userLimits.dailyBudget) {
    throw new Error('Daily budget exceeded');
  }

  return handler();
}
```

## Security Practices

### Input Validation & Sanitization

<Warning>
Always validate and sanitize user inputs to prevent prompt injection and abuse.
</Warning>

<Accordion title="Prompt Injection Protection">
  ```typescript
  class PromptSanitizer {
    private dangerousPatterns = [
      /ignore.*previous.*instructions?/i,
      /forget.*instructions?/i,
      /system.*prompt/i,
      /\[INST\]|\[\/INST\]/g, // Common instruction markers
      /<\|.*\|>/g, // Special tokens
    ];

    private maxPromptLength = 2000;

    sanitize(prompt: string): string {
      // Remove dangerous patterns
      let sanitized = prompt;
      for (const pattern of this.dangerousPatterns) {
        sanitized = sanitized.replace(pattern, '[FILTERED]');
      }

      // Limit length
      if (sanitized.length > this.maxPromptLength) {
        sanitized = sanitized.slice(0, this.maxPromptLength) + '...';
      }

      // Remove HTML tags
      sanitized = sanitized.replace(/<[^>]*>/g, '');

      // Normalize whitespace
      sanitized = sanitized.replace(/\s+/g, ' ').trim();

      return sanitized;
    }

    validate(prompt: string): { isValid: boolean; reason?: string } {
      if (!prompt || prompt.trim().length === 0) {
        return { isValid: false, reason: 'Empty prompt' };
      }

      if (prompt.length > this.maxPromptLength) {
        return { isValid: false, reason: 'Prompt too long' };
      }

      // Check for suspicious patterns
      for (const pattern of this.dangerousPatterns) {
        if (pattern.test(prompt)) {
          return { isValid: false, reason: 'Potentially malicious content' };
        }
      }

      return { isValid: true };
    }
  }

  // Usage in API route
  const sanitizer = new PromptSanitizer();

  export const POST = withAuthentication(async (session, req) => {
    const { prompt } = await req.json();

    // Validate input
    const validation = sanitizer.validate(prompt);
    if (!validation.isValid) {
      throw new Error(`Invalid prompt: ${validation.reason}`);
    }

    // Sanitize input
    const sanitizedPrompt = sanitizer.sanitize(prompt);

    // Continue with generation...
  });
  ```
</Accordion>

<Accordion title="Rate Limiting & Abuse Prevention">
  ```typescript
  import { Ratelimit } from '@upstash/ratelimit';
  import { Redis } from '@upstash/redis';

  // Create rate limiter
  const ratelimit = new Ratelimit({
    redis: Redis.fromEnv(),
    limiter: Ratelimit.slidingWindow(10, '60 s'), // 10 requests per minute
    analytics: true,
  });

  // Advanced rate limiting with user tiers
  class TieredRateLimit {
    private limits = {
      free: { requests: 10, window: '60s' },
      pro: { requests: 100, window: '60s' },
      enterprise: { requests: 1000, window: '60s' }
    };

    async checkLimit(userId: string, tier: keyof typeof this.limits) {
      const limit = this.limits[tier];
      const identifier = `${tier}:${userId}`;

      const { success, limit: remainingLimit, reset } =
        await ratelimit.limit(identifier);

      if (!success) {
        throw new Error(`Rate limit exceeded. Reset in ${reset}ms`);
      }

      return { remaining: remainingLimit, reset };
    }
  }
  ```
</Accordion>

### Content Filtering

Implement content filtering to prevent harmful outputs:

```typescript
class ContentFilter {
  private harmfulPatterns = [
    /violence|violent|kill|murder|harm/i,
    /hate|racist|discrimination/i,
    // Add more patterns based on your content policy
  ];

  async filterResponse(text: string): Promise<{
    filtered: string;
    flagged: boolean;
    reasons: string[];
  }> {
    const reasons: string[] = [];
    let flagged = false;

    // Check for harmful patterns
    for (const pattern of this.harmfulPatterns) {
      if (pattern.test(text)) {
        flagged = true;
        reasons.push('Potentially harmful content detected');
        break;
      }
    }

    // External content moderation service (optional)
    const moderationResult = await this.externalModeration(text);
    if (moderationResult.flagged) {
      flagged = true;
      reasons.push(...moderationResult.categories);
    }

    const filtered = flagged ? this.sanitizeHarmfulContent(text) : text;

    return { filtered, flagged, reasons };
  }

  private async externalModeration(text: string) {
    // Example: OpenAI Moderation API
    try {
      const response = await fetch('https://api.openai.com/v1/moderations', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ input: text }),
      });

      const result = await response.json();
      return {
        flagged: result.results[0].flagged,
        categories: Object.keys(result.results[0].categories).filter(
          key => result.results[0].categories[key]
        )
      };
    } catch (error) {
      console.error('Moderation API error:', error);
      return { flagged: false, categories: [] };
    }
  }

  private sanitizeHarmfulContent(text: string): string {
    // Replace harmful content with safe alternatives
    return "I apologize, but I cannot provide that type of content. Please try a different request.";
  }
}
```

## Error Handling & Resilience

### Comprehensive Error Handling

<Accordion title="Retry Logic with Exponential Backoff">
  ```typescript
  class AIGenerationService {
    private maxRetries = 3;
    private baseDelay = 1000; // 1 second

    async generateWithRetry(
      prompt: string,
      options: any = {},
      attempt = 1
    ): Promise<string> {
      try {
        const { text } = await generateText({
          model: openai(options.model || 'gpt-3.5-turbo'),
          prompt,
          ...options
        });

        return text;
      } catch (error) {
        console.error(`Generation attempt ${attempt} failed:`, error);

        if (attempt >= this.maxRetries) {
          throw new Error(`Generation failed after ${this.maxRetries} attempts: ${error.message}`);
        }

        // Exponential backoff
        const delay = this.baseDelay * Math.pow(2, attempt - 1);
        await this.sleep(delay);

        // Retry with potentially different model on certain errors
        if (this.isModelUnavailableError(error) && attempt === 1) {
          options.model = 'gpt-3.5-turbo'; // Fallback model
        }

        return this.generateWithRetry(prompt, options, attempt + 1);
      }
    }

    private sleep(ms: number): Promise<void> {
      return new Promise(resolve => setTimeout(resolve, ms));
    }

    private isModelUnavailableError(error: any): boolean {
      return error.status === 503 ||
             error.message?.includes('model unavailable') ||
             error.message?.includes('overloaded');
    }
  }
  ```
</Accordion>

<Accordion title="Circuit Breaker Pattern">
  ```typescript
  class AICircuitBreaker {
    private failures = 0;
    private lastFailureTime = 0;
    private state: 'closed' | 'open' | 'half-open' = 'closed';

    private readonly failureThreshold = 5;
    private readonly recoveryTimeout = 30000; // 30 seconds

    async execute<T>(operation: () => Promise<T>): Promise<T> {
      if (this.state === 'open') {
        if (Date.now() - this.lastFailureTime < this.recoveryTimeout) {
          throw new Error('Circuit breaker is open - service unavailable');
        }

        this.state = 'half-open';
      }

      try {
        const result = await operation();
        this.onSuccess();
        return result;
      } catch (error) {
        this.onFailure();
        throw error;
      }
    }

    private onSuccess(): void {
      this.failures = 0;
      this.state = 'closed';
    }

    private onFailure(): void {
      this.failures++;
      this.lastFailureTime = Date.now();

      if (this.failures >= this.failureThreshold) {
        this.state = 'open';
      }
    }

    getStatus() {
      return {
        state: this.state,
        failures: this.failures,
        lastFailureTime: this.lastFailureTime
      };
    }
  }

  // Usage
  const circuitBreaker = new AICircuitBreaker();

  async function safeGenerate(prompt: string): Promise<string> {
    return circuitBreaker.execute(async () => {
      const { text } = await generateText({
        model: openai('gpt-4'),
        prompt
      });
      return text;
    });
  }
  ```
</Accordion>

## Monitoring & Analytics

### Performance Monitoring

Track and analyze AI generation performance:

<Accordion title="Performance Metrics Collection">
  ```typescript
  interface GenerationMetrics {
    requestId: string;
    userId: string;
    model: string;
    prompt: string;
    inputTokens: number;
    outputTokens: number;
    latency: number;
    cost: number;
    success: boolean;
    errorType?: string;
    timestamp: Date;
  }

  class AIMetricsCollector {
    async recordGeneration(metrics: GenerationMetrics): Promise<void> {
      // Store in database
      await db.aiMetrics.create({ data: metrics });

      // Send to analytics service
      await this.sendToAnalytics(metrics);

      // Real-time monitoring alerts
      await this.checkAlerts(metrics);
    }

    private async sendToAnalytics(metrics: GenerationMetrics): Promise<void> {
      // Example: Send to analytics service
      try {
        await fetch('https://analytics.example.com/ai-metrics', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(metrics)
        });
      } catch (error) {
        console.error('Analytics reporting failed:', error);
      }
    }

    private async checkAlerts(metrics: GenerationMetrics): Promise<void> {
      // High latency alert
      if (metrics.latency > 10000) { // 10 seconds
        await this.sendAlert('HIGH_LATENCY', metrics);
      }

      // High cost alert
      if (metrics.cost > 1.0) { // $1
        await this.sendAlert('HIGH_COST', metrics);
      }

      // Error rate monitoring
      const recentErrorRate = await this.getRecentErrorRate();
      if (recentErrorRate > 0.1) { // 10%
        await this.sendAlert('HIGH_ERROR_RATE', { errorRate: recentErrorRate });
      }
    }
  }
  ```
</Accordion>

### Usage Analytics Dashboard

Create insights from AI usage data:

```typescript
class AIAnalytics {
  async getUserInsights(userId: string, timeframe: 'week' | 'month' = 'month') {
    const since = timeframe === 'week'
      ? new Date(Date.now() - 7 * 24 * 60 * 60 * 1000)
      : new Date(Date.now() - 30 * 24 * 60 * 60 * 1000);

    const metrics = await db.aiMetrics.findMany({
      where: {
        userId,
        timestamp: { gte: since }
      }
    });

    return {
      totalGenerations: metrics.length,
      totalTokens: metrics.reduce((sum, m) => sum + m.inputTokens + m.outputTokens, 0),
      totalCost: metrics.reduce((sum, m) => sum + m.cost, 0),
      averageLatency: metrics.reduce((sum, m) => sum + m.latency, 0) / metrics.length,
      successRate: metrics.filter(m => m.success).length / metrics.length,
      popularModels: this.getModelUsageStats(metrics),
      trends: await this.getUsageTrends(userId, since)
    };
  }

  private getModelUsageStats(metrics: GenerationMetrics[]) {
    const modelCounts = metrics.reduce((acc, m) => {
      acc[m.model] = (acc[m.model] || 0) + 1;
      return acc;
    }, {} as Record<string, number>);

    return Object.entries(modelCounts)
      .sort(([,a], [,b]) => b - a)
      .map(([model, count]) => ({ model, count }));
  }
}
```

## Production Deployment

### Environment Configuration

<Tabs>
  <Tab title="Environment Variables">
    ```bash .env.production
    # AI Provider API Keys
    OPENAI_API_KEY=sk-your-production-key
    ANTHROPIC_API_KEY=sk-ant-your-production-key

    # Rate Limiting
    REDIS_URL=redis://your-redis-instance
    UPSTASH_REDIS_REST_URL=https://your-upstash-instance
    UPSTASH_REDIS_REST_TOKEN=your-token

    # Monitoring
    ANALYTICS_API_KEY=your-analytics-key
    ERROR_TRACKING_DSN=your-error-tracking-dsn

    # Feature Flags
    ENABLE_STREAMING=true
    ENABLE_CONTENT_FILTERING=true
    DEFAULT_MODEL=gpt-3.5-turbo

    # Cost Controls
    MAX_MONTHLY_COST_PER_USER=100
    MAX_DAILY_COST_PER_USER=10
    ```
  </Tab>

  <Tab title="Docker Configuration">
    ```dockerfile
    FROM node:18-alpine

    WORKDIR /app

    COPY package*.json ./
    RUN npm ci --only=production

    COPY . .
    RUN npm run build

    # Security: Run as non-root user
    RUN addgroup -g 1001 -S nodejs
    RUN adduser -S nextjs -u 1001
    USER nextjs

    EXPOSE 3000

    ENV NODE_ENV=production
    ENV NEXT_TELEMETRY_DISABLED=1

    CMD ["npm", "start"]
    ```
  </Tab>

  <Tab title="Health Checks">
    ```typescript
    // Health check endpoint
    export async function GET() {
      const checks = await Promise.allSettled([
        // AI service health
        checkAIServiceHealth(),

        // Database connectivity
        checkDatabaseHealth(),

        // Cache service
        checkCacheHealth(),

        // Rate limiter
        checkRateLimiterHealth()
      ]);

      const healthy = checks.every(check => check.status === 'fulfilled');

      return Response.json(
        {
          status: healthy ? 'healthy' : 'degraded',
          checks: checks.map((check, index) => ({
            service: ['ai', 'database', 'cache', 'ratelimiter'][index],
            status: check.status
          })),
          timestamp: new Date().toISOString()
        },
        { status: healthy ? 200 : 503 }
      );
    }
    ```
  </Tab>
</Tabs>

### Scaling Considerations

<CardGroup cols={2}>
  <Card
    title="Horizontal Scaling"
    icon="server"
  >
    Design stateless services that can scale horizontally across multiple instances.
  </Card>
  <Card
    title="Load Balancing"
    icon="shuffle"
  >
    Distribute AI requests across multiple providers and regions for reliability.
  </Card>
  <Card
    title="Queue Management"
    icon="clock"
  >
    Use job queues for long-running generations to prevent timeouts.
  </Card>
  <Card
    title="CDN Integration"
    icon="globe"
  >
    Cache static responses and assets at edge locations.
  </Card>
</CardGroup>

<Note>
These best practices have been battle-tested in production environments. Start with the basics and gradually implement more advanced patterns as your application scales.
</Note>

## Next Steps

<CardGroup cols={2}>
  <Card
    title="Try the Playground"
    icon="play"
    href="/playground/generate-text"
  >
    Apply these best practices in our interactive playground environment.
  </Card>
  <Card
    title="Architecture Guide"
    icon="sitemap"
    href="/architecture/api-architecture"
  >
    Understand how these patterns integrate with the application architecture.
  </Card>
  <Card
    title="API Reference"
    icon="book"
    href="/ai-sdk/generate-text/api-reference"
  >
    Explore detailed API documentation and configuration options.
  </Card>
  <Card
    title="Examples"
    icon="code"
    href="/ai-sdk/generate-text/examples"
  >
    See these best practices implemented in real-world code examples.
  </Card>
</CardGroup>