---
title: "AI SDK Best Practices"
description: "Production optimization strategies for AI applications including performance, cost management, and security"
---

# AI SDK Best Practices

## What

Production-ready strategies for optimizing AI-powered applications including performance optimization, cost management, security practices, and error handling patterns.

## Why

- **Performance**: Faster response times and better user experience
- **Cost Control**: Optimize AI usage and reduce API costs
- **Security**: Protect user data and prevent abuse
- **Reliability**: Robust error handling and graceful degradation

## How

### Performance Optimization

#### 1. Model Selection
Choose the right model for your use case:

```typescript
// Fast and cost-effective for simple tasks
const quickResponse = await generateText({
  model: openai('gpt-3.5-turbo'),
  prompt: 'Summarize this in one sentence: ...',
});

// Higher quality for complex tasks
const detailedResponse = await generateText({
  model: openai('gpt-4'),
  prompt: 'Write a detailed analysis of...',
});
```

#### 2. Streaming for Better UX
Always use streaming for user-facing applications:

```typescript
// app/api/ai/chat/route.ts
import { streamText } from 'ai';

export const POST = withAuthentication(async (session, request) => {
  const { messages } = await request.json();

  const result = await streamText({
    model: openai('gpt-3.5-turbo'),
    messages,
  });

  return result.toDataStreamResponse();
});
```

#### 3. Response Caching
Cache common responses to reduce API calls:

```typescript
import { unstable_cache } from 'next/cache';

const getCachedResponse = unstable_cache(
  async (prompt: string) => {
    const { text } = await generateText({
      model: openai('gpt-3.5-turbo'),
      prompt,
    });
    return text;
  },
  ['ai-response'],
  { revalidate: 3600 } // Cache for 1 hour
);
```

### Cost Management

#### 1. Token Limits
Set appropriate limits to control costs:

```typescript
const { text } = await generateText({
  model: openai('gpt-3.5-turbo'),
  prompt: userPrompt,
  maxTokens: 150, // Limit response length
});
```

#### 2. Request Validation
Validate inputs to prevent expensive requests:

```typescript
export const POST = withAuthentication(async (session, request) => {
  const { prompt } = await parseRequestBody<{ prompt: string }>(request);

  // Validate prompt length
  if (prompt.length > 1000) {
    throw new ApiError(400, 'Prompt too long', 'PROMPT_TOO_LONG');
  }

  // Check for repeated requests
  const hash = createHash('md5').update(prompt).digest('hex');
  const cached = await getCachedResponse(hash);
  if (cached) return { text: cached };

  const { text } = await generateText({
    model: openai('gpt-3.5-turbo'),
    prompt,
  });

  await setCachedResponse(hash, text);
  return { text };
});
```

#### 3. Usage Tracking
Monitor AI usage per user:

```typescript
// Track usage in database
const logAIUsage = async (userId: string, model: string, tokens: number) => {
  await db.insert(aiUsage).values({
    userId,
    model,
    tokens,
    timestamp: new Date(),
  });
};

export const POST = withAuthentication(async (session, request) => {
  const { prompt } = await request.json();

  const result = await generateText({
    model: openai('gpt-3.5-turbo'),
    prompt,
  });

  // Log usage
  await logAIUsage(
    session.user.id,
    'gpt-3.5-turbo',
    result.usage.totalTokens
  );

  return { text: result.text };
});
```

### Security Best Practices

#### 1. Input Sanitization
Always sanitize user inputs:

```typescript
import DOMPurify from 'dompurify';

const sanitizePrompt = (prompt: string): string => {
  // Remove potentially harmful content
  const clean = DOMPurify.sanitize(prompt);

  // Additional validation
  if (clean.includes('ignore previous instructions')) {
    throw new ApiError(400, 'Invalid prompt', 'INVALID_PROMPT');
  }

  return clean;
};

export const POST = withAuthentication(async (session, request) => {
  const { prompt } = await request.json();
  const cleanPrompt = sanitizePrompt(prompt);

  // Use sanitized prompt
  const { text } = await generateText({
    model: openai('gpt-3.5-turbo'),
    prompt: cleanPrompt,
  });

  return { text };
});
```

#### 2. Rate Limiting
Implement per-user rate limits:

```typescript
import { Ratelimit } from '@upstash/ratelimit';
import { Redis } from '@upstash/redis';

const ratelimit = new Ratelimit({
  redis: Redis.fromEnv(),
  limiter: Ratelimit.slidingWindow(10, '1 m'), // 10 requests per minute
});

export const POST = withAuthentication(async (session, request) => {
  const { success } = await ratelimit.limit(session.user.id);

  if (!success) {
    throw new ApiError(429, 'Too many requests', 'RATE_LIMITED');
  }

  // Process request...
});
```

#### 3. Content Filtering
Filter AI responses for inappropriate content:

```typescript
const filterResponse = (text: string): string => {
  // Implement content filtering logic
  const sensitiveTerms = ['inappropriate', 'harmful'];

  for (const term of sensitiveTerms) {
    if (text.toLowerCase().includes(term)) {
      return 'I apologize, but I cannot provide that type of content.';
    }
  }

  return text;
};
```

## Example

Complete production-ready AI endpoint:

```typescript
// app/api/ai/generate-text/route.ts
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { withAuthentication, withAITelemetry } from '@/lib/api/base';
import { ratelimit, sanitizePrompt, filterResponse } from '@/lib/ai/utils';

export const POST = withAuthentication(async (session, request) => {
  // Rate limiting
  const { success } = await ratelimit.limit(session.user.id);
  if (!success) {
    throw new ApiError(429, 'Too many requests', 'RATE_LIMITED');
  }

  // Parse and validate
  const { prompt } = await parseRequestBody<{ prompt: string }>(request);
  validateRequiredFields(body, ['prompt']);

  // Input sanitization
  const cleanPrompt = sanitizePrompt(prompt);

  // Check cache first
  const cacheKey = createHash('md5').update(cleanPrompt).digest('hex');
  const cached = await getCachedResponse(cacheKey);
  if (cached) return { text: cached, cached: true };

  // Generate with telemetry
  const { text, usage } = await generateText(
    withAITelemetry({
      model: openai('gpt-3.5-turbo'),
      prompt: cleanPrompt,
      maxTokens: 150,
    }, {
      functionId: 'generate-text',
      metadata: {
        userId: session.user.id,
        promptLength: cleanPrompt.length,
      },
    })
  );

  // Filter response
  const filteredText = filterResponse(text);

  // Cache result
  await setCachedResponse(cacheKey, filteredText);

  // Log usage
  await logAIUsage(session.user.id, 'gpt-3.5-turbo', usage.totalTokens);

  return { text: filteredText };
});
```

## Key Recommendations

### Development
- **Start Simple**: Begin with basic text generation, add complexity gradually
- **Use TypeScript**: Leverage full type safety for better development experience
- **Test Thoroughly**: Test with various inputs including edge cases

### Production
- **Monitor Usage**: Track tokens, costs, and performance metrics
- **Implement Fallbacks**: Handle API failures gracefully
- **User Feedback**: Collect user feedback to improve AI responses

### Security
- **Never Trust User Input**: Always validate and sanitize prompts
- **Implement Rate Limits**: Prevent abuse and control costs
- **Content Filtering**: Review AI outputs for inappropriate content

### Performance
- **Choose Right Models**: Balance cost vs quality for each use case
- **Cache Common Responses**: Reduce API calls with intelligent caching
- **Use Streaming**: Provide immediate feedback for better UX